{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f79592",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd11e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "APPNAME = 'Test'\n",
    "EXE_INSTANCES = 5\n",
    "EXE_MEM = '5G'\n",
    "EXE_CORES = 1\n",
    "SPARK_MASTER = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217653f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/06 21:54:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First SparkContext:\n",
      "APP Name : SparkExample1\n",
      "Master : local\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .master('local') \\\n",
    "                    .appName('SparkExample1') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "print(\"First SparkContext:\");\n",
    "print(\"APP Name :\", spark.sparkContext.appName);\n",
    "print(\"Master :\", spark.sparkContext.master);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d56187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mics-air.fios-router.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkExample1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=SparkExample1>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baaf3565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df= spark.createDataFrame(sc.parallelize(range(1,6)).map(lambda i: Row(int_column=i, square_int_column=str(i**2)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27b4b3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|int_column|square_int_column|\n",
      "+----------+-----------------+\n",
      "|         1|                1|\n",
      "|         2|                4|\n",
      "|         3|                9|\n",
      "|         4|               16|\n",
      "|         5|               25|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5aece936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "true\n",
      "1.4004638195037842\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "start = time.time()\n",
    "# ############### save a DataFrame as compressed CSV format ###############\n",
    "# --------- 1 ---------\n",
    "df.write.mode(\"overwrite\").csv('file:///Users/mic/Documents/df', compression=\"gzip\", mode='overwrite') # Spark 2.0\n",
    "# output - part-00000-490921ca-e77b-45d5-8e13-32b55bf4a19e-c000.csv.gz\n",
    "\n",
    "# --------- 2 ---------\n",
    "print(os.path.exists(\"/Users/mic/Documents/df2\"))\n",
    "if os.path.exists(\"/Users/mic/Documents/df2\"):\n",
    "    print('true')\n",
    "    import shutil\n",
    "    shutil.rmtree(\"/Users/mic/Documents/df2\")\n",
    "    df.write.option(\"compression\",\"gzip\").csv(\"file:///Users/mic/Documents/df2\") # Spark 2.2+\n",
    "    # output - part-00000-89419a91-9797-4767-b6a1-c4849beb97a7-c000.csv.gz\n",
    "else:\n",
    "    print('false')\n",
    "#     os.removedirs(\"/Users/mic/Documents/df2\")\n",
    "    df.write.option(\"compression\",\"gzip\").csv(\"file:///Users/mic/Documents/df2\") # Spark 2.2+\n",
    "    \n",
    "    \n",
    "################ save a DataFrame as compressed parquet format ###############\n",
    "# --------- 3 ---------\n",
    "# df.write.format(\"parquet\").mode(\"overwrite\").save(\"file:///Users/mic/Documents/df3.parquet\") \n",
    "df.write.mode(\"overwrite\").option(\"compression\",\"gzip\").parquet(\"file:///Users/mic/Documents/df3\") # Spark 2.0\n",
    "# output - part-00000-80e9da75-dcb0-4fdf-b8f7-8e20a3670e50-c000.gz.parquet\n",
    "\n",
    "# ---------4 ---------\n",
    "df.write.mode(\"overwrite\").csv(\"file:///Users/mic/Documents/df4\")  # Spark 2.2+\n",
    "# output - part-00000-7d883bc6-b4a8-44bb-9a67-67d49585c3f8-c000.csv\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0b16e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 328\r\n",
      "drwxr-xr-x@ 14 mic  staff     448 Feb 27 21:01 \u001b[34myumichelle.github.io\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 mic  staff  163989 Mar  6 17:12 Untitled-1.ipynb\r\n",
      "drwxr-xr-x   6 mic  staff     192 Mar  6 21:57 \u001b[34mdf3.parquet\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   6 mic  staff     192 Mar  6 22:11 \u001b[34mdf\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   6 mic  staff     192 Mar  6 22:11 \u001b[34mdf2\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   6 mic  staff     192 Mar  6 22:11 \u001b[34mdf3\u001b[m\u001b[m\r\n",
      "drwxr-xr-x   6 mic  staff     192 Mar  6 22:11 \u001b[34mdf4\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  10 mic  staff     320 Mar  6 22:12 \u001b[34mbrowsable-api\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr /Users/mic/Documents/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74911281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_column</th>\n",
       "      <th>square_int_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   int_column square_int_column\n",
       "0           1                 1\n",
       "1           2                 4\n",
       "2           3                 9\n",
       "3           4                16\n",
       "4           5                25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How to read parquet file compressed by .gz\n",
    "import pandas as pd\n",
    "# df = pd.read_parquet(\"file:///Users/mic/Documents/df3.parquet\")\n",
    "# display(df)\n",
    "\n",
    "# df_read_parquet = pd.read_parquet('/Users/mic/Documents/df3.parquet', engine='pyarrow')\n",
    "# display(df_read_parquet)\n",
    "# print(type(df_read_parquet))\n",
    "\n",
    "\n",
    "import gzip\n",
    "data = pd.read_parquet(\"/Users/mic/Documents/df3/part-00000-23029649-ae34-45ed-9807-9c51b37a9107-c000.gz.parquet\")\n",
    "\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dae18fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|int_column|square_int_column|\n",
      "+----------+-----------------+\n",
      "|         1|                1|\n",
      "|         2|                4|\n",
      "|         3|                9|\n",
      "|         4|               16|\n",
      "|         5|               25|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parquetFile = spark.read.parquet(\"/Users/mic/Documents/df3\")\n",
    "parquetFile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed4f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert parquet to csv\n",
    "# after df = spark.read.parquet(\"/path/to/infile.parquet\")\n",
    "print(type(parquetFile))\n",
    "\n",
    "parquetFile.write.csv(\"/Users/mic/Documents/outfile.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54304fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcsv = pd.read_csv('/Users/mic/Documents/outfile.csv/part-00000-7d883bc6-b4a8-44bb-9a67-67d49585c3f8-c000.csv') \n",
    "dfcsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd88510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 ('my_env': venv)",
   "language": "python",
   "name": "python389jvsc74a57bd01da678d51418a55d2cc169fc504900297ec149eb8d173085d19ae14a1b8b9285"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
